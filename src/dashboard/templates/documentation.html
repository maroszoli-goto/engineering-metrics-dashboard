{% extends "content_page.html" %}

{% block title %}Documentation{% endblock %}
{% block page_title %}Documentation{% endblock %}

{% block header_title %}üìö Documentation & FAQ{% endblock %}

{% block breadcrumb_items %}
<a href="/documentation">Documentation</a>
{% endblock %}

{% block extra_css %}
<style>
    .faq-section {
        margin-bottom: 40px;
    }
    .faq-item {
        margin-bottom: 30px;
    }
    .faq-question {
        font-size: 18px;
        font-weight: 600;
        color: var(--accent-primary);
        margin-bottom: 10px;
    }
    .faq-answer {
        line-height: 1.6;
        color: var(--text-primary);
    }
    .faq-answer code {
        background: var(--bg-tertiary);
        padding: 2px 6px;
        border-radius: 3px;
        font-family: monospace;
    }
    .faq-answer ul {
        margin: 10px 0;
        padding-left: 30px;
    }
    .faq-answer ol {
        margin: 10px 0;
        padding-left: 30px;
    }
</style>
{% endblock %}

{% block main_content %}
<!-- Data Collection Methods -->
<div class="card">
    <h2>üìä Data Collection Methods</h2>

    <div class="faq-section">
        <h3>GitHub Metrics</h3>

        <div class="faq-item">
            <div class="faq-question">How are Pull Requests counted?</div>
            <div class="faq-answer">
                PRs are counted by creation date within the time window. Each PR created by a team member is counted once, regardless of merge status. Metrics include:
                <ul>
                    <li><strong>Total PRs</strong>: All PRs created in the period</li>
                    <li><strong>Merged PRs</strong>: PRs that were successfully merged</li>
                    <li><strong>Merge Rate</strong>: (Merged PRs / Total PRs) √ó 100</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Commits counted?</div>
            <div class="faq-answer">
                Commits are extracted from Pull Requests, not from the default branch. This ensures consistency:
                <ul>
                    <li>Only commits that are part of PRs are counted</li>
                    <li>Commits are attributed by GitHub username (not Git author name)</li>
                    <li>Each unique commit SHA is counted once (deduplicated if appearing in multiple PRs)</li>
                    <li>Team members must have commits ‚â• PRs (since 1 PR = at least 1 commit)</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Code Reviews counted?</div>
            <div class="faq-answer">
                Reviews are counted when submitted within the time window. Each review submission (comment, approval, or request changes) is counted once per PR. Multiple reviews on the same PR by the same person count separately.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Cycle Time?</div>
            <div class="faq-answer">
                Cycle Time measures the duration from PR creation to merge/close. It's calculated as:
                <ul>
                    <li><strong>For merged PRs</strong>: Time from PR creation to merge</li>
                    <li><strong>For closed PRs</strong>: Time from PR creation to close</li>
                    <li><strong>Average Cycle Time</strong>: Mean of all completed PRs in the period</li>
                </ul>
                Lower cycle times indicate faster code review and merge processes.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Lines of Code counted?</div>
            <div class="faq-answer">
                Lines Added and Lines Deleted are aggregated from all commits made by the person. These come from GitHub's diff statistics and include:
                <ul>
                    <li>All code changes (additions, deletions, modifications)</li>
                    <li>Configuration files, tests, and documentation</li>
                    <li>Generated code and dependencies if committed</li>
                </ul>
                Note: High line counts don't always mean higher productivity - quality matters more than quantity.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>Jira Metrics</h3>

        <div class="faq-item">
            <div class="faq-question">How are Jira issues counted?</div>
            <div class="faq-answer">
                Jira issues are fetched using JQL queries with date filtering:
                <ul>
                    <li><strong>Completed</strong>: Issues with resolution date within the configured time period</li>
                    <li><strong>In Progress</strong>: Issues currently unresolved (no resolution date)</li>
                    <li>Issues must be assigned to the team member's Jira username</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How does the system filter out noise from administrative updates?</div>
            <div class="faq-answer">
                The system uses optimized Jira queries to prevent bulk administrative operations (like mass label updates) from polluting metrics:
                <ul>
                    <li><strong>Query Logic</strong>: <code>assignee = "user" AND (created >= -Xd OR resolved >= -Xd OR (statusCategory != Done AND updated >= -Xd))</code></li>
                    <li><strong>What it captures</strong>:
                        <ul>
                            <li>New issues created in your selected date range</li>
                            <li>Old issues resolved in your selected date range</li>
                            <li>Active work (statusCategory != Done) updated in your selected date range</li>
                        </ul>
                    </li>
                    <li><strong>What it filters out</strong>: Closed/Done tickets that only had administrative updates (labels, comments, attachments)</li>
                    <li><strong>Why it matters</strong>: A bulk label update on 5,000+ closed tickets won't pollute your metrics anymore</li>
                    <li><strong>Note</strong>: The time window (Xd) dynamically matches your selected date range (30d, 60d, 90d, 180d, 365d, or a specific year)</li>
                </ul>
                This ensures cleaner metrics and better query performance while maintaining 100% accuracy for actual work.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Throughput?</div>
            <div class="faq-answer">
                Throughput measures team velocity by counting issues completed per week within your selected date range. It's calculated as:
                <ul>
                    <li>Total issues completed √∑ Number of weeks in range = Weekly Average</li>
                    <li>Only counts issues with resolution dates in that period</li>
                    <li>Higher throughput indicates faster delivery</li>
                    <li>The calculation adapts to your selected date range (30d, 90d, 365d, etc.)</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is WIP (Work In Progress)?</div>
            <div class="faq-answer">
                WIP counts currently active issues:
                <ul>
                    <li><strong>WIP Count</strong>: Number of unresolved issues</li>
                    <li><strong>Average WIP Age</strong>: How long items have been in progress</li>
                    <li><strong>WIP by Status</strong>: Distribution across statuses (In Progress, Code Review, etc.)</li>
                </ul>
                Lower WIP counts and ages generally lead to faster completion.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What are Flagged/Blocked Issues?</div>
            <div class="faq-answer">
                Issues marked as blocked or flagged need attention:
                <ul>
                    <li>Fetched using team-specific Jira filters</li>
                    <li>Shows assignee and days blocked</li>
                    <li>Removing blockers maintains team flow</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Bugs tracked?</div>
            <div class="faq-answer">
                Bug metrics track product quality:
                <ul>
                    <li><strong>Created</strong>: New bugs filed in the period</li>
                    <li><strong>Resolved</strong>: Bugs fixed in the period</li>
                    <li><strong>Net</strong>: Created - Resolved (negative is good)</li>
                </ul>
                Trend charts show history matching your selected date range to identify patterns.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Jira Cycle Time?</div>
            <div class="faq-answer">
                Jira Cycle Time measures issue resolution speed:
                <ul>
                    <li>Time from issue creation to resolution</li>
                    <li>Calculated in hours, displayed in hours</li>
                    <li>Average across all completed issues</li>
                </ul>
            </div>
        </div>
    </div>
</div>

<!-- DORA Metrics -->
<div class="card">
    <h2>üìà DORA Metrics</h2>

    <div class="faq-section">
        <h3>What are DORA Metrics?</h3>
        <div class="faq-answer">
            DORA (DevOps Research and Assessment) metrics are industry-standard measurements of software delivery performance. These four key metrics help teams understand their deployment velocity, stability, and incident response capabilities. Performance is classified into four levels: <strong>Elite</strong>, <strong>High</strong>, <strong>Medium</strong>, and <strong>Low</strong> based on research from thousands of organizations.
        </div>
    </div>

    <div class="faq-section">
        <h3>1. Deployment Frequency</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                How often your team deploys code to production. Measured as deployments per week.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                Uses <strong>Jira Fix Versions</strong> (not GitHub Releases) to track deployments:
                <ul>
                    <li>Releases matching pattern "Live - D/MMM/YYYY" are counted as production</li>
                    <li>Other patterns (e.g., "Staging") are filtered out</li>
                    <li>Deployments are counted within the 90-day window</li>
                    <li>Rate calculated as: (Total deployments / Days) √ó 7</li>
                </ul>
                A weekly trend chart shows deployment volume over time.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> More than 7 deployments per week (on-demand deployment)</li>
                    <li><strong>High:</strong> 1-7 deployments per week</li>
                    <li><strong>Medium:</strong> 1-4 deployments per month</li>
                    <li><strong>Low:</strong> Less than 1 deployment per month</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>2. Lead Time for Changes</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                Time from code commit to running in production. Measures how quickly changes move through your delivery pipeline.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                Uses a two-tier approach:
                <ol>
                    <li><strong>Jira-based (preferred):</strong> PR includes Jira issue key ‚Üí Issue mapped to Fix Version ‚Üí Time from PR merge to Fix Version release date</li>
                    <li><strong>Time-based (fallback):</strong> Time from PR merge to next deployment after merge</li>
                </ol>
                <p>The median lead time across all PRs in the period is displayed. PRs without Jira issues use the fallback method.</p>
                <p><strong>Cross-Team Filtering:</strong> Releases are automatically filtered to only include deployments where the team has assigned issues. This prevents lead time contamination from other teams' releases in multi-team projects.</p>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 1 day</li>
                    <li><strong>High:</strong> 1-7 days</li>
                    <li><strong>Medium:</strong> 1-4 weeks</li>
                    <li><strong>Low:</strong> More than 4 weeks</li>
                </ul>
                A weekly trend chart shows median lead time over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>3. Change Failure Rate (CFR)</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                Percentage of deployments that cause production incidents. Indicates deployment stability and quality.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                <strong>Requires incident tracking setup (see below).</strong>
                <ul>
                    <li>Incidents are fetched from your Jira <code>incidents</code> filter</li>
                    <li>Incidents correlated to deployments using two methods:
                        <ul>
                            <li><strong>Direct:</strong> Incident has Fix Version matching deployment name</li>
                            <li><strong>Time-based:</strong> Incident created within 24 hours after deployment</li>
                        </ul>
                    </li>
                    <li>Calculation: (Failed deployments / Total deployments) √ó 100</li>
                </ul>
                Shows "‚Äî" if no incidents in period (which is excellent!).
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 15% (fewer than 1 in 7 deployments fail)</li>
                    <li><strong>High:</strong> 15-16%</li>
                    <li><strong>Medium:</strong> 16-30%</li>
                    <li><strong>Low:</strong> More than 30%</li>
                </ul>
                A weekly trend chart shows failure rate percentage over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>4. Mean Time to Recovery (MTTR)</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                How quickly your team recovers from production incidents. Measured as median time from incident creation to resolution.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                <strong>Requires incident tracking setup (see below).</strong>
                <ul>
                    <li>Incidents fetched from your Jira <code>incidents</code> filter</li>
                    <li>Resolution time = (Resolved timestamp - Created timestamp)</li>
                    <li>Median of all resolved incidents is displayed</li>
                    <li>P95 (95th percentile) also shown for outlier awareness</li>
                </ul>
                Shows "‚Äî" if no resolved incidents in period.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 1 hour</li>
                    <li><strong>High:</strong> 1-24 hours (less than 1 day)</li>
                    <li><strong>Medium:</strong> 1-7 days</li>
                    <li><strong>Low:</strong> More than 7 days</li>
                </ul>
                A weekly trend chart shows median recovery time over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>Setting Up Incident Tracking</h3>
        <div class="faq-answer">
            <p><strong>CFR and MTTR require incident tracking configuration.</strong></p>
            <p>To enable these metrics:</p>
            <ol>
                <li>Create a Jira filter for production incidents using JQL like:
                    <code style="display: block; margin: 10px 0; padding: 10px; background: var(--bg-tertiary);">
                        project IN (YOUR_PROJECTS)<br>
                        AND issuetype IN ("Incident", "GCS Escalation")<br>
                        AND (created >= -90d OR resolved >= -90d)
                    </code>
                </li>
                <li>Add the filter ID to your team configuration in <code>config/config.yaml</code>:
                    <code style="display: block; margin: 10px 0; padding: 10px; background: var(--bg-tertiary);">
                        jira:<br>
                        &nbsp;&nbsp;filters:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;incidents: 12345  # Your filter ID
                    </code>
                </li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
            </ol>
            <p>See <strong>README.md</strong> for detailed setup instructions and JQL examples.</p>
        </div>
    </div>

    <div class="faq-section">
        <h3>Overall DORA Performance Level</h3>
        <div class="faq-answer">
            Your team's overall DORA level is determined by the <strong>lowest-performing metric</strong> among the four. For example, if you have Elite deployment frequency but Low lead time, your overall level is Low. This encourages balanced improvement across all dimensions of software delivery performance.
        </div>
    </div>

    <div class="faq-section">
        <h3>üöÄ How do I compare DORA metrics across teams?</h3>
        <div class="faq-answer">
            <p>Navigate to the <strong>Team Comparison</strong> page to see side-by-side DORA metrics for all teams.</p>

            <h4>DORA Metrics Included:</h4>
            <ul>
                <li><strong>Performance Level Badges:</strong> Visual Elite/High/Medium/Low classification for each team</li>
                <li><strong>Deployment Frequency:</strong> Deployments per week with total deployment counts</li>
                <li><strong>Lead Time for Changes:</strong> Median days from commit to production</li>
                <li><strong>Change Failure Rate:</strong> Percentage of deployments causing incidents</li>
                <li><strong>Mean Time to Recovery:</strong> Median days to resolve production incidents</li>
            </ul>

            <h4>Understanding the Difference Column:</h4>
            <ul>
                <li><strong>Green values:</strong> First team is performing better</li>
                <li><strong>Red values:</strong> Second team is performing better</li>
                <li><strong>Deployment Frequency:</strong> Higher is better (more deployments = better)</li>
                <li><strong>Lead Time, CFR, MTTR:</strong> Lower is better (faster recovery, fewer failures = better)</li>
            </ul>

            <p><strong>Tip:</strong> Use the difference column to quickly identify performance gaps between teams and focus improvement efforts where they matter most.</p>
        </div>
    </div>
</div>

<!-- Time Windows -->
<div class="card">
    <h2>‚è∞ Time Windows</h2>

    <div class="faq-item">
        <div class="faq-question">What time periods are used?</div>
        <div class="faq-answer">
            Different dashboards use different time windows:
            <ul>
                <li><strong>All Metrics (Team, Person, GitHub, Jira)</strong>: Fixed 90-day rolling window</li>
                <li><strong>Jira Throughput</strong>: Dynamically calculated from completed issues within your selected date range</li>
                <li><strong>Jira Trends</strong>: Last 90 days for bugs/scope charts</li>
            </ul>
            <p style="margin-top: 10px;">
                <strong>Note:</strong> All metrics use a consistent 90-day window defined by the <code>DAYS_BACK</code> constant in the collection script.
                This ensures fair comparison across teams and individuals.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How often is data refreshed?</div>
        <div class="faq-answer">
            Data collection runs on a schedule:
            <ul>
                <li><strong>Automatic</strong>: Daily at 10:00 AM via launchd scheduler</li>
                <li><strong>Manual</strong>: Run <code>python collect_data.py</code></li>
                <li><strong>Duration</strong>: 15-30 minutes per collection</li>
                <li><strong>Cache</strong>: Stored in <code>data/metrics_cache.pkl</code></li>
            </ul>
            Dashboard shows cache timestamp at top of page.

            <p style="margin-top: 15px;"><strong>Reload Data Button:</strong></p>
            <p>When you click "üîÑ Reload Data" in the hamburger menu, the button shows a loading state:</p>
            <ul>
                <li>Icon changes to ‚è≥ (hourglass)</li>
                <li>Text changes to "Reloading..."</li>
                <li>Button is disabled to prevent multiple clicks</li>
                <li>Page automatically refreshes when complete</li>
            </ul>
        </div>
    </div>
</div>

<!-- API Information -->
<div class="card">
    <h2>üîå API Details</h2>

    <div class="faq-item">
        <div class="faq-question">Which GitHub API is used?</div>
        <div class="faq-answer">
            We use <strong>GitHub GraphQL API v4</strong> for efficiency:
            <ul>
                <li>Separate rate limit (5000 points/hour)</li>
                <li>50-70% fewer API calls than REST</li>
                <li>Single query fetches PRs + reviews + commits</li>
            </ul>
            See <code>src/collectors/github_graphql_collector.py</code> for implementation.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How is Jira accessed?</div>
        <div class="faq-answer">
            We use <strong>Jira REST API</strong> with Bearer token authentication:
            <ul>
                <li>Team-specific filter IDs configured per team</li>
                <li>JQL queries with date filtering</li>
                <li>SSL verification disabled for self-signed certificates</li>
            </ul>
            Use <code>python list_jira_filters.py</code> to discover filter IDs.
        </div>
    </div>
</div>

<!-- Common Questions -->
<div class="card">
    <h2>‚ùì Common Questions</h2>

    <div class="faq-item">
        <div class="faq-question">Why does someone show 0 commits but have PRs?</div>
        <div class="faq-answer">
            This shouldn't happen after the commit counting fix, but possible causes:
            <ul>
                <li>Git author name doesn't match GitHub username</li>
                <li>Commits were made by someone else but PR opened by this person</li>
                <li>Data collection issue - try re-running collection</li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">Why does someone show 0 Jira issues?</div>
        <div class="faq-answer">
            Possible reasons:
            <ul>
                <li>No Jira username mapping in config.yaml</li>
                <li>No issues resolved in the last 90 days</li>
                <li>Jira username doesn't match configuration</li>
                <li>Issues assigned to different username/account</li>
            </ul>
            Check Jira username mapping in <code>config/config.yaml</code>.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How do I add a new team member?</div>
        <div class="faq-answer">
            Update <code>config/config.yaml</code>:
            <ol>
                <li>Add GitHub username to <code>github.members</code> list</li>
                <li>Add Jira username to <code>jira.members</code> list</li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
                <li>Restart dashboard: <code>launchctl restart com.team-metrics.dashboard</code></li>
            </ol>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">Can I change the time window?</div>
        <div class="faq-answer">
            To change the collection time window:
            <ul>
                <li>Edit the <code>DAYS_BACK</code> constant in <code>collect_data.py</code> (line 19)</li>
                <li>Default is 90 days - change to any value (e.g., 30, 60, 180)</li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
                <li>Restart the dashboard to load new data</li>
            </ul>
            <p style="margin-top: 10px;">
                <strong>Note:</strong> All metrics (team, person, Jira) will use the same time window.
                There is no per-dashboard or per-person time window selection in the UI.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">What does "Merge Rate" mean?</div>
        <div class="faq-answer">
            Merge Rate = (Merged PRs √∑ Total PRs) √ó 100%
            <ul>
                <li>100% = All PRs were merged</li>
                <li>&lt;100% = Some PRs were closed without merging</li>
                <li>Higher is generally better, but depends on team practices</li>
            </ul>
        </div>
    </div>
</div>

<!-- Multi-Environment Support -->
<div class="card">
    <h2>üåê Multi-Environment Support</h2>

    <div class="faq-item">
        <div class="faq-question">What is multi-environment support?</div>
        <div class="faq-answer">
            <p>The dashboard can connect to multiple Jira environments (Production, UAT, Staging, etc.)
            with separate configurations for each.</p>

            <h4>Key Features:</h4>
            <ul>
                <li><strong>Environment Selector:</strong> Switch between environments in the hamburger menu</li>
                <li><strong>Separate Credentials:</strong> Each environment has its own Jira server, credentials, and filter IDs</li>
                <li><strong>Time Offset:</strong> Automatically adjust for UAT databases that are behind production</li>
                <li><strong>Isolated Cache:</strong> Data is stored separately per environment</li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How do I switch environments in the dashboard?</div>
        <div class="faq-answer">
            <p>Use the <strong>Environment Selector</strong> in the hamburger menu (top-left):</p>
            <ol>
                <li>Click the hamburger menu icon</li>
                <li>Find the "üåç Environment" dropdown</li>
                <li>Select "UAT" or "Production"</li>
                <li>The page will reload with the selected environment's data</li>
            </ol>

            <p>The current environment is shown in the badge at the top-right:</p>
            <ul>
                <li><span class="badge badge-prod">‚úÖ PROD</span> - Production environment</li>
                <li><span class="badge badge-uat">‚ö†Ô∏è UAT</span> - UAT environment</li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How do I collect data from different environments?</div>
        <div class="faq-answer">
            <p>Use the <code>--env</code> flag when collecting data:</p>

            <pre><code># Collect from UAT environment
python collect_data.py --date-range 90d --env uat

# Collect from production
python collect_data.py --date-range 90d --env prod

# Use environment variable
export TEAM_METRICS_ENV=uat
python collect_data.py --date-range 90d</code></pre>

            <p>Each environment creates separate cache files:</p>
            <pre><code>data/metrics_cache_90d_prod.pkl
data/metrics_cache_90d_uat.pkl</code></pre>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">What is time offset and when do I need it?</div>
        <div class="faq-answer">
            <p>UAT/staging environments often contain older data (e.g., 6 months behind production).
            The <code>time_offset_days</code> setting automatically adjusts queries and dates.</p>

            <h4>How It Works:</h4>
            <p>If UAT is 180 days (6 months) behind production:</p>
            <ul>
                <li><strong>Query Adjustment:</strong> Queries look 270 days back (90 + 180 offset) to find UAT data</li>
                <li><strong>Date Display:</strong> Dates are shifted forward so they align with production timeline</li>
                <li><strong>Example:</strong> UAT shows "Querying UAT data from: Apr 29, 2025 - Jul 28, 2025"</li>
            </ul>

            <p>Configure in <code>config.yaml</code>:</p>
            <pre><code>jira:
  environments:
    uat:
      server: "https://jira-uat.company.com"
      time_offset_days: 180  # 6 months behind</code></pre>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How do I configure multiple environments?</div>
        <div class="faq-answer">
            <p>Update <code>config/config.yaml</code> with environment-specific settings:</p>

            <pre><code>jira:
  default_environment: "prod"

  environments:
    prod:
      server: "https://jira.company.com"
      username: "username"
      api_token: "prod_token"
      project_keys: ["RSC", "RW"]
      time_offset_days: 0

    uat:
      server: "https://jira-uat.company.com"
      username: "username"
      api_token: "uat_token"
      project_keys: ["RSC", "RW"]
      time_offset_days: 180

teams:
  - name: "Native"
    jira:
      filters:
        prod:
          wip: 81010
          bugs: 81015
        uat:
          wip: 80517
          bugs: 80510</code></pre>

            <p>Each team needs separate filter IDs for each environment.</p>
            <p>See <code>config/config.example.yaml</code> for complete examples.</p>
        </div>
    </div>
</div>

<!-- Technical Details -->
<div class="card">
    <h2>‚öôÔ∏è Technical Details</h2>

    <div class="faq-item">
        <div class="faq-question">Where is data stored?</div>
        <div class="faq-answer">
            All collected data is cached locally:
            <ul>
                <li><code>data/metrics_cache.pkl</code> - Main metrics cache (pickle format)</li>
                <li><code>logs/collect_data.log</code> - Collection logs</li>
                <li><code>logs/dashboard.log</code> - Dashboard logs</li>
            </ul>
            Cache includes: teams, persons, comparison data, and timestamp.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How are metrics calculated?</div>
        <div class="faq-answer">
            Calculation pipeline:
            <ol>
                <li>Raw data collected as lists of dicts</li>
                <li>Converted to pandas DataFrames</li>
                <li>Aggregated by <code>MetricsCalculator</code> in <code>src/models/metrics.py</code></li>
                <li>Cached to disk as pickle</li>
                <li>Loaded by Flask and passed to Jinja templates</li>
            </ol>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">What happens if collection fails?</div>
        <div class="faq-answer">
            Dashboard continues using old cache until successful collection:
            <ul>
                <li>Check <code>logs/collect_data.log</code> for errors</li>
                <li>Verify API tokens in <code>config/config.yaml</code></li>
                <li>Check API rate limits (GitHub/Jira)</li>
                <li>Manually trigger: <code>python collect_data.py</code></li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üß≠ How do I navigate between different views?</div>
        <div class="faq-answer">
            Use the hamburger menu (‚ò∞) in the top-right corner of any page. The menu provides:
            <ul>
                <li><strong>üè† Home:</strong> Return to the main teams overview</li>
                <li><strong>üìö Documentation:</strong> View this help page</li>
                <li><strong>üåì Theme Toggle:</strong> Switch between light and dark modes</li>
            </ul>
            The menu automatically closes when you select an option or click outside of it.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üé® What do the chart colors mean?</div>
        <div class="faq-answer">
            Charts use a consistent semantic color system across all pages:
            <ul>
                <li><strong style="color: #e74c3c;">üî¥ Red:</strong> Items created or added (bugs filed, scope added)</li>
                <li><strong style="color: #2ecc71;">üü¢ Green:</strong> Items resolved or completed (bugs fixed, scope delivered)</li>
                <li><strong style="color: #3498db;">üîµ Blue:</strong> Net difference (created minus resolved)</li>
            </ul>
            <strong>Trend Charts:</strong> Top panel shows created vs resolved, bottom panel shows net difference.<br>
            <strong>Tip:</strong> Positive net difference (blue trending up) indicates backlog growth; negative indicates backlog reduction.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üë• How do I compare team members?</div>
        <div class="faq-answer">
            View side-by-side comparison of all team members:
            <ul>
                <li>Navigate to a team dashboard (click on team card from homepage)</li>
                <li>Open the hamburger menu (‚ò∞ icon in the breadcrumbs bar)</li>
                <li>Click "üë• Compare Members"</li>
                <li>See leaderboard with top performers (ü•áü•àü•â badges)</li>
                <li>View detailed metrics table with best values highlighted in green</li>
                <li>Compare PRs, reviews, commits, Jira metrics, and more</li>
            </ul>

            <h3 style="margin-top: 30px;">Performance Scoring System</h3>
            <p>
                The dashboard uses a <strong>composite performance scoring system</strong> to rank teams and individuals.
                Scores range from <strong>0-100</strong> (higher is better) and are calculated using min-max normalization
                across all teams/members.
            </p>

            <h4>Metric Weights:</h4>
            <table style="width: 100%; margin: 20px 0; border-collapse: collapse;">
                <thead>
                    <tr style="background: var(--bg-tertiary);">
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Metric</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Weight</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">PRs Created</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Pull requests authored</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Reviews Given</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Code reviews provided to others</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Commits</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">15%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Total commits authored</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Cycle Time</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">15%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">PR merge time (lower is better, inverted)</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Jira Completed</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Jira issues resolved</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Merge Rate</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">10%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Percentage of PRs successfully merged</td>
                    </tr>
                </tbody>
            </table>

            <h4>Key Features:</h4>
            <ul style="margin-top: 15px;">
                <li><strong>Cycle Time Inversion:</strong> Lower cycle times score higher (faster PR merges are better)</li>
                <li><strong>Team Size Normalization:</strong> Divides volume metrics by team size for fair per-capita comparison</li>
                <li><strong>Consistent Algorithm:</strong> Same scoring logic for both team and person comparisons</li>
                <li><strong>Visual Rankings:</strong> Teams/members displayed with scores, ranks, and badges (ü•áü•àü•â)</li>
            </ul>

            <p style="margin-top: 20px;">
                <strong>Where you see it:</strong>
            </p>
            <ul>
                <li>Team Comparison page: Overall Performance card with scores per team</li>
                <li>Team Member Comparison: Top Performers leaderboard with rankings</li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">‚ö° Performance Optimizations</div>
        <div class="faq-answer">
            <p>
                The system includes several automatic performance optimizations that work behind the scenes to ensure fast data collection:
            </p>

            <h3 style="margin-top: 20px;">1. Parallel Collection (4-5x speedup)</h3>
            <p>
                Data is collected concurrently across multiple teams, repositories, people, and Jira filters.
                This reduces collection time from ~5 minutes to ~1.5 minutes.
            </p>
            <ul style="margin-top: 10px;">
                <li><strong>Team Parallelization:</strong> Up to 3 teams collected simultaneously</li>
                <li><strong>Repository Parallelization:</strong> Up to 5 repos per team collected simultaneously</li>
                <li><strong>Person Parallelization:</strong> Up to 8 people's metrics collected simultaneously</li>
                <li><strong>Filter Parallelization:</strong> Up to 4 Jira filters per team collected simultaneously</li>
            </ul>

            <h3 style="margin-top: 20px;">2. HTTP Connection Pooling (5-10% speedup)</h3>
            <p>
                Reuses HTTP connections to reduce network overhead. Instead of creating a new TCP connection
                for every API request, connections are pooled and reused automatically.
            </p>

            <h3 style="margin-top: 20px;">3. Repository Caching (5-15 seconds saved)</h3>
            <p>
                Team repository lists are cached for 24 hours to eliminate redundant GitHub API queries.
                Since repository lists rarely change, this optimization saves time on every collection.
            </p>

            <h3 style="margin-top: 20px;">4. GraphQL Query Batching (20-40% speedup)</h3>
            <p>
                Combines multiple GitHub API queries into single requests. Instead of making separate queries
                for PRs and Releases, a single batched query fetches both, reducing API calls by 50%.
            </p>

            <div class="info-box" style="background: var(--bg-tertiary); padding: 15px; margin-top: 20px; border-left: 4px solid #3498db; border-radius: 4px;">
                <strong>üìù Note:</strong> All performance optimizations are automatic and require no configuration.
                The system applies them intelligently to ensure fast, efficient data collection.
            </div>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">‚öôÔ∏è How do I customize Performance Score weights?</div>
        <div class="faq-answer">
            <p>
                You can customize the weights used to calculate performance scores by visiting the
                <strong><a href="/settings" style="color: var(--accent-primary);">Settings page</a></strong>.
                Access it through the hamburger menu (‚ò∞) or by clicking the link above.
            </p>

            <h4>Available Metrics (10 total):</h4>
            <p>Performance scores are calculated from 10 weighted metrics across three categories:</p>

            <p><strong>GitHub Metrics (60%):</strong></p>
            <ul>
                <li><strong>Pull Requests</strong> - Higher is better (default: 15%)</li>
                <li><strong>Code Reviews</strong> - Higher is better (default: 15%)</li>
                <li><strong>Commits</strong> - Higher is better (default: 10%)</li>
                <li><strong>Cycle Time</strong> - Lower is better (default: 10%)</li>
                <li><strong>Merge Rate</strong> - Higher is better (default: 5%)</li>
            </ul>

            <p><strong>DORA Metrics (30%):</strong></p>
            <ul>
                <li><strong>Deployment Frequency</strong> - Higher is better (default: 10%)</li>
                <li><strong>Lead Time</strong> - Lower is better (default: 10%)</li>
                <li><strong>Change Failure Rate</strong> - Lower is better (default: 5%)</li>
                <li><strong>MTTR</strong> - Lower is better (default: 5%)</li>
            </ul>

            <p><strong>Jira Metrics (10%):</strong></p>
            <ul>
                <li><strong>Jira Completed</strong> - Higher is better (default: 15%)</li>
            </ul>

            <h4>Quick Presets:</h4>
            <p>Choose from 4 predefined weight configurations (all include DORA metrics):</p>
            <ul>
                <li><strong>‚öñÔ∏è Balanced:</strong> Equal emphasis on all metrics
                    <ul style="margin-top: 5px; color: var(--text-secondary); font-size: 0.9em;">
                        <li>PRs: 15%, Reviews: 15%, Commits: 10%, Cycle Time: 10%, Jira: 15%, Merge Rate: 5%</li>
                        <li>Deploy Freq: 10%, Lead Time: 10%, CFR: 5%, MTTR: 5%</li>
                    </ul>
                </li>
                <li><strong>üéØ Code Quality:</strong> Focus on reviews, merge rate, and deployment stability
                    <ul style="margin-top: 5px; color: var(--text-secondary); font-size: 0.9em;">
                        <li>PRs: 10%, Reviews: 25%, Commits: 5%, Cycle Time: 10%, Jira: 10%, Merge Rate: 15%</li>
                        <li>Deploy Freq: 5%, Lead Time: 10%, CFR: 5%, MTTR: 5%</li>
                    </ul>
                </li>
                <li><strong>üöÄ Velocity:</strong> Emphasize output, delivery speed, and deployment frequency
                    <ul style="margin-top: 5px; color: var(--text-secondary); font-size: 0.9em;">
                        <li>PRs: 20%, Reviews: 10%, Commits: 15%, Cycle Time: 5%, Jira: 20%, Merge Rate: 5%</li>
                        <li>Deploy Freq: 15%, Lead Time: 5%, CFR: 3%, MTTR: 2%</li>
                    </ul>
                </li>
                <li><strong>üìã Jira Focus:</strong> Prioritize Jira completion and reliability
                    <ul style="margin-top: 5px; color: var(--text-secondary); font-size: 0.9em;">
                        <li>PRs: 10%, Reviews: 10%, Commits: 5%, Cycle Time: 5%, Jira: 30%, Merge Rate: 10%</li>
                        <li>Deploy Freq: 10%, Lead Time: 10%, CFR: 5%, MTTR: 5%</li>
                    </ul>
                </li>
            </ul>

            <h4>Custom Weights:</h4>
            <p>Adjust individual metric weights using interactive sliders:</p>
            <ul>
                <li>Drag sliders to set weights from 0% to 100% for each of the 10 metrics</li>
                <li>Metrics are grouped into sections: GitHub metrics, DORA metrics, and Jira metrics</li>
                <li>Total must equal exactly 100% to save</li>
                <li>Real-time feedback shows current total and validation</li>
            </ul>

            <h4>Impact:</h4>
            <p>Custom weights affect performance scores displayed on:</p>
            <ul>
                <li><strong>Team Comparison:</strong> Overall Performance rankings</li>
                <li><strong>Team Member Comparison:</strong> Top Performers leaderboard</li>
            </ul>

            <h4>Reset Option:</h4>
            <p>
                Click <strong>"Reset"</strong> button to restore default (Balanced) weights.
                A confirmation dialog will appear before resetting.
            </p>

            <h4>Configuration Options:</h4>
            <p>You can configure performance weights in two ways:</p>
            <ol>
                <li><strong>Settings Page (Recommended):</strong> Use the interactive UI at <code>/settings</code> with sliders and presets</li>
                <li><strong>Config File:</strong> Add <code>performance_weights</code> section to <code>config/config.yaml</code></li>
            </ol>

            <p><strong>Config File Example:</strong></p>
            <pre style="background: var(--bg-tertiary); padding: 15px; border-radius: 6px; overflow-x: auto;"><code>performance_weights:
  # GitHub metrics
  prs: 0.15
  reviews: 0.15
  commits: 0.10
  cycle_time: 0.10
  merge_rate: 0.05
  jira_completed: 0.15
  # DORA metrics
  deployment_frequency: 0.10
  lead_time: 0.10
  change_failure_rate: 0.05
  mttr: 0.05</code></pre>

            <p><strong>Notes:</strong></p>
            <ul>
                <li>If <code>performance_weights</code> is not in config, system uses built-in defaults</li>
                <li>All 10 metric weights must be specified and sum to 1.0 (100%)</li>
                <li>Settings page changes override config file settings</li>
                <li>Old config files with only 6 metrics automatically use new defaults with warning</li>
            </ul>

            <p style="margin-top: 15px; padding: 15px; background: var(--bg-tertiary); border-radius: 8px; border-left: 4px solid var(--accent-primary);">
                <strong>üí° Tip:</strong> Experiment with different presets to find what best reflects your team's priorities.
                Changes take effect immediately and apply to all performance score calculations.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üìä How do I export metrics data?</div>
        <div class="faq-answer">
            <p>
                Export metrics data to CSV or JSON format for offline analysis, reporting, or integration with other tools.
                Export buttons are available on all major dashboard pages.
            </p>

            <h4>Available Pages:</h4>
            <ul>
                <li><strong>Team Dashboard:</strong> Export complete team metrics including GitHub, DORA, and Jira data</li>
                <li><strong>Person Dashboard:</strong> Export individual contributor metrics and trends</li>
                <li><strong>Team Comparison:</strong> Export side-by-side comparison of all teams</li>
                <li><strong>Team Member Comparison:</strong> Export performance rankings and member metrics</li>
            </ul>

            <h4>Export Formats:</h4>
            <table style="width: 100%; margin: 20px 0; border-collapse: collapse;">
                <thead>
                    <tr style="background: var(--bg-tertiary);">
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Format</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Data Structure</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);"><strong>CSV</strong></td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Flattened with dot notation (e.g., <code>github.pr_count</code>)</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Excel, Google Sheets, spreadsheet analysis</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);"><strong>JSON</strong></td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Nested structure with full metadata</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">API integration, programmatic access, data pipelines</td>
                    </tr>
                </tbody>
            </table>

            <h4>Filename Format:</h4>
            <p>All exported files include the current date in the filename for easy tracking:</p>
            <ul>
                <li><code>team_native_metrics_2026-01-14.csv</code></li>
                <li><code>person_jdoe_metrics_2026-01-14.json</code></li>
                <li><code>team_comparison_metrics_2026-01-14.csv</code></li>
            </ul>
            <p>This makes it easy to maintain historical snapshots and organize archived exports.</p>

            <h4>What's Included:</h4>
            <ul>
                <li><strong>Team Exports:</strong> GitHub metrics (PRs, reviews, commits, cycle time), DORA metrics (deployment frequency, lead time, CFR, MTTR), Jira metrics (throughput, WIP, bugs, incidents), member breakdown with individual performance scores</li>
                <li><strong>Person Exports:</strong> Individual GitHub metrics, Jira issue completion data, weekly activity trends</li>
                <li><strong>Comparison Exports:</strong> Multi-team data with normalized per-capita metrics, performance scores, team rankings</li>
                <li><strong>Metadata:</strong> All exports include export timestamp, date range information, and entity identifiers</li>
            </ul>

            <h4>How to Use:</h4>
            <ol>
                <li>Navigate to any dashboard page (team, person, or comparison view)</li>
                <li>Locate the export buttons at the top of the page</li>
                <li>Click <strong>"üìä Export CSV"</strong> for spreadsheet format or <strong>"üìã Export JSON"</strong> for API format</li>
                <li>File downloads automatically with a descriptive name (e.g., <code>team_native_metrics.csv</code>)</li>
            </ol>

            <h4>Common Use Cases:</h4>
            <ul>
                <li><strong>Custom Analysis:</strong> Import CSV into Excel/Google Sheets for pivot tables, charts, and custom calculations</li>
                <li><strong>Historical Tracking:</strong> Archive snapshots of metrics over time for trend analysis</li>
                <li><strong>Stakeholder Reports:</strong> Share formatted data with management or external teams</li>
                <li><strong>Data Integration:</strong> Use JSON exports to feed metrics into BI tools, data warehouses, or custom dashboards</li>
                <li><strong>Compliance & Auditing:</strong> Maintain records of team performance and DORA metrics for reporting requirements</li>
            </ul>

            <p style="margin-top: 15px; padding: 15px; background: var(--bg-tertiary); border-radius: 8px; border-left: 4px solid var(--accent-primary);">
                <strong>üí° Tip:</strong> CSV exports are flattened for spreadsheet compatibility, while JSON exports preserve the full nested structure. Choose the format that best fits your workflow.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">‚Üë What is the Back to Top button?</div>
        <div class="faq-answer">
            <p>A floating button that appears in the bottom-right corner when you scroll down long pages (team dashboards, person dashboards, comparison views).</p>

            <p><strong>How it works:</strong></p>
            <ul>
                <li>Button appears automatically after scrolling 300px down the page</li>
                <li>Click it to smoothly scroll back to the top</li>
                <li>Button disappears when you're at the top of the page</li>
                <li>Responsive: Adjusts size on mobile devices</li>
            </ul>

            <p><strong>Tip:</strong> Use this for quick navigation instead of scrolling manually on pages with many charts.</p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">‚úÖ How do I validate my configuration?</div>
        <div class="faq-answer">
            <p>Run the configuration validation tool before collecting data or starting the dashboard:</p>

            <pre><code>python validate_config.py</code></pre>

            <p><strong>What it checks:</strong></p>
            <ul>
                <li>Config file exists and is valid YAML</li>
                <li>GitHub token format is correct</li>
                <li>Jira server URL is valid (http:// or https://)</li>
                <li>All teams have required fields (name, members)</li>
                <li>Each member has GitHub and Jira usernames</li>
                <li>No duplicate team names</li>
                <li>Performance weights sum to 100%</li>
                <li>Dashboard settings are valid (port, timeouts)</li>
            </ul>

            <p><strong>Example output:</strong></p>
            <pre><code>Validating config: config/config.yaml
--------------------------------------------------

‚ö†Ô∏è  WARNINGS:
  ‚Ä¢ Team 'Backend' member 'john' missing 'jira' (optional but recommended)

‚úÖ Config validation passed!
   (1 warning(s))</code></pre>

            <p><strong>Tip:</strong> Run validation after editing <code>config.yaml</code> to catch errors before they cause collection failures.</p>
        </div>
    </div>
</div>
{% endblock %}
